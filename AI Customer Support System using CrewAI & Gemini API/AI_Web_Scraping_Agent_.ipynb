{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHeuSCq_uVy2",
    "outputId": "a2157c8b-9b1d-48af-ad98-e462074c24b5"
   },
   "outputs": [],
   "source": [
    "!pip install -qU crewai langchain_openai 'crewai[tools]' google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46WooK1suaPQ",
    "outputId": "8ab6fb33-689a-4717-9cf9-a98bfec6630c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Ask the user to enter the API keys (they won't show up on screen)\n",
    "os.environ[\"GEMINI_API_KEY\"] = getpass(\"Enter your Gemini API key: \")\n",
    "os.environ[\"SERPER_API_KEY\"] = getpass(\"Enter your Serper API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfLfg_dtKBFB"
   },
   "source": [
    "###Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whOXjfQRueMV"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from crewai import LLM\n",
    "\n",
    "Gemini = LLM(\n",
    "    model=\"gemini/gemini-2.0-flash\",)\n",
    "\n",
    "\n",
    "#gpt4o = ChatOpenAI(model = \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGO9Gr0mAJpu"
   },
   "source": [
    "### AI Web Scraping Agent üîç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zm9_pd42ujm9",
    "outputId": "2c085e39-535c-489e-e21e-68cc1d077b1e"
   },
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import SerperDevTool\n",
    "\n",
    "# Initialize tools\n",
    "search_tool = SerperDevTool()\n",
    "\n",
    "def create_web_scraping_agent(url, target_data):\n",
    "    # Define the Web Scraping Agent\n",
    "    scraping_agent = Agent(\n",
    "        role=\"Data Collector\",\n",
    "        goal=f\"Collect and analyze {target_data} from {url}\",\n",
    "        backstory=\"An expert data analyst trained to gather and process web information efficiently\",\n",
    "        verbose=True,\n",
    "        tools=[search_tool],\n",
    "        llm=Gemini\n",
    "    )\n",
    "\n",
    "    # Define the Data Processing Agent\n",
    "    processing_agent = Agent(\n",
    "        role=\"Data Processor\",\n",
    "        goal=f\"Process and structure the collected {target_data}\",\n",
    "        backstory=\"A data processing specialist who organizes and formats collected information into useful insights\",\n",
    "        verbose=True,\n",
    "        tools=[search_tool],\n",
    "        llm=Gemini\n",
    "    )\n",
    "\n",
    "    # Define the Data Collection Task\n",
    "    scraping_task = Task(\n",
    "        description=f\"Search and collect {target_data} related to {url}\",\n",
    "        expected_output=f\"Raw collected data about {target_data} from {url}\",\n",
    "        agent=scraping_agent\n",
    "    )\n",
    "\n",
    "    # Define the Data Processing Task\n",
    "    processing_task = Task(\n",
    "        description=f\"Process and structure the collected {target_data} into a clear format\",\n",
    "        expected_output=f\"A structured and cleaned summary of {target_data}\",\n",
    "        agent=processing_agent\n",
    "    )\n",
    "\n",
    "    # Create and Run the Crew\n",
    "    crew = Crew(\n",
    "        agents=[scraping_agent, processing_agent],\n",
    "        tasks=[scraping_task, processing_task],\n",
    "        verbose=True,\n",
    "        process=Process.sequential\n",
    "    )\n",
    "\n",
    "    result = crew.kickoff(inputs={\"url\": url, \"target_data\": target_data})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn-xzy89utDY"
   },
   "source": [
    "### Execute the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "77f5e364e05d4c09b9eb39c71c70b7d4",
      "ac0b2289c8aa44fdacbe2a5e7a11554d",
      "15d7a00f27344b798a211c16f9748ae5",
      "9a695a0e57134b6da1dc6e0d32d0c200",
      "e721e9a57c9645bc9909039d9467a377",
      "7437deaad3914f83bd7fe93c72049619",
      "81440a9064ed459fb2d26d48d6de46e3",
      "295c2e1803bd4bb294b9661562a4c7d3",
      "e45516d64de24994a32c51bedeb74f16",
      "2cff70aaf7b6495ebd248d16b87e5a16",
      "c648a78414374f05a3e8c87a8591e48c",
      "123c8c3c5546406f89dfc78c1e8b9567",
      "35f59694ff634e58bb9e7d850d7d40b1",
      "69e5608521e4462090e30c5a1e9f89c8"
     ]
    },
    "id": "CsM7G9QXunWB",
    "outputId": "b51516af-a51e-4305-cf9b-983a08c8fa1f"
   },
   "outputs": [],
   "source": [
    "url = input(\"Enter the website URL to scrape: \")\n",
    "target_data = input(\"Enter the type of data to extract (e.g., product prices, news headlines): \")\n",
    "scraping_result = create_web_scraping_agent(url, target_data)\n",
    "print(\"Web Scraping Results:\")\n",
    "print(scraping_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97BohH1y2mes"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
